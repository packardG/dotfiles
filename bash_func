# Main ssh func, copies over dotfiles. Ignores .git and other hidden files
function gssh() {
    ssh $1 "mkdir ~/.packard_dotfiles 2>/dev/null"
    scp -qr ~/.packard_dotfiles/[!.]* $1:~/.packard_dotfiles/
    ssh -tAx $1 "bash --rcfile ~/.packard_dotfiles/bashrc"
}

function sumissh() {
    ssh gilly@sumi "mkdir ~/.packard_dotfiles 2>/dev/null"
    scp -qr ~/.packard_dotfiles/[!.]* gilly@sumi:~/.packard_dotfiles/
    ssh -tAx gilly@sumi "bash --rcfile ~/.packard_dotfiles/bashrc"
}

######################################################
# Vertica Functions
######################################################

# Helpful installer function for vertica
function install_rpm() {
  /scratch_b/qa/vertica/QA/VT_Scenario/run_qa_scenario --install_vertica -L --rpm_dir="$1" --ignore_install_fail
}

# Easy connect to db
function pvsql() {
  /opt/vertica/bin/vsql -p `admintools -t list_db -d "$1" | awk '/^Port/{print $2}'`
}

# Either tail vertica log, or open in vim
function vlog() {
    if [ "$1" = "-t" ]; then
        taillog=$(admintools -t list_db -d "${@:2}" | awk '/^Catalog/{print $3}')
        if [ -e ${taillog}/vertica.log ]; then
            tail -f "$taillog"/vertica.log
        else
            tail -f "$taillog"/vertica*log*
        fi
    else
        log=$(admintools -t list_db -d "$1" | awk '/^Catalog/{print $3}')
        if [ -e ${log}/vertica.log ]; then
            vim "$log"/vertica.log
        else
            vim "$log"/vertica*log*
        fi
    fi
}

# Jump to catalog dir
function catdir() {
    cd `admintools -t list_db -d "$1" | awk '/^Catalog/{print $3}'`
}

# Create 'regular' database
function pcreate() {
  /opt/vertica/bin/admintools -u -t create_db -d "$1" -s $HOSTLIST
}

# Nimbus create db func.
function ncreate() {
  /opt/vertica/bin/admintools -u -t create_db -d "$1" -s $HOSTLIST --shard-count="$2" --communal-storage-location=s3://nimbusdb/gpackard/"$1" --depot-path=/scratch_b/qa/"$1"_depot/
}

# Start db
function pstart() {
  /opt/vertica/bin/admintools -t start_db -d "$1"
}

# Stop db
function pstop() {
  /opt/vertica/bin/admintools -t stop_db -d "$1"
}

# Stop && Drop db
function pdrop() {
  /opt/vertica/bin/admintools -t stop_db -d "$1"
  /opt/vertica/bin/admintools -t drop_db -d "$1"
}

function pwipe() {
  /opt/vertica/bin/vsql -p`cat /scratch_b/qa/"$1"/port.dat` -f $TOOLS/mhayden/remove_all_tables_views_schemas.sql -d"$1"
}

# Check if vertica db processes still running on all hosts in cluster
function dbps() {
    for host in ${HOSTLIST//,/ }
    do
        echo "---------------------------------------------------------------"
        echo "Host: $host"
        echo "---------------------------------------------------------------"
        ssh "$host" "ps -ef | grep $1 | grep vertica | grep -v ssh | grep -v grep"
    done
}

# Check disk space on all hosts in cluster.
# Useful for stress clusters
function dbdf() {
    for host in ${HOSTLIST//,/ }
    do
        echo "---------------------------------------------------------------"
        echo "Host: $host"
        echo "---------------------------------------------------------------"
        ssh "$host" "df -ah /vertica/data"
    done
}

# Run command across all hosts in cluster
function dbfunc() {
    for host in ${HOSTLIST//,/ }
    do
        echo "---------------------------------------------------------------"
        echo "Host: $host"
        echo "---------------------------------------------------------------"
        ssh "$host" "$@"
    done
}

# Function for kicking off stress tests in Nimbus on EC2
function run_stress() {
    cd $STRESS_VT
    nohup ./manage_stress_tests.pl --suite_name "$1" --cluster_hosts $HOSTLIST --failure_behavior=StopPassedDatabases --shard_count=6 --communal_storage=s3://nimbusdb/gpackard/stress/"$1"/$$ &
}

function stress_cluster() {
    /scratch_b/qa/vertica/QA/tools/nimbus/qa_run_nimbus.sh -n 6 -s Stress_VT &
}

######################################################
# Misc. Functions
######################################################
function tf() {
    tail -f "$1"
}

function mkdircd() {
    mkdir "$1" && cd "$1"
}

# pstree func.
function vtree() {
    pstree -alp $(whoami)
}

# Why did I put this in here....
function spongebob() {
    echo "$@" | tr -d ' ' | sed 's/\(.\)\(.\)/\u\1 \l\2 /g'
}

# Dump timestamp and avg mem usage into csv
function memsar() {
    sar -r | grep -v Linux | grep -v Average | awk '{if (NR!=1) {print}}' | awk '{printf $1","$5"\n";}' | awk '{if (NR!=1) {print}}' > test.csv
}

######################################################
# AWS Functions
######################################################

# Get qa stress bucket tags for easy finger-pointing or deletion
function get_bucket_tags() {
    for bucket in `aws s3api list-buckets --query "Buckets[*].Name" | grep qastress | tr -d \",`
    do
        tags=$(aws s3api get-bucket-tagging --bucket $bucket --query "TagSet[]" --output text | awk '{printf "{"$1": " $2"}"}')
        echo $bucket '|' $tags
    done
}

# Get all untagged buckets for easy finger-pointing or deletion
function get_untagged_buckets() {
    for bucket in `aws s3api list-buckets --query "Buckets[*].Name" | tr -d \",[]`
    do
        region=$(aws s3api get-bucket-location --bucket $bucket --output text 2>/dev/null)
        # For some reason this returns 'None' for us-east-1. Maybe because it is my default in configure?
        # Possible this could give different results for other users depending on their configured default region, will have to test
        if [ "$region" == "None" ];then
            region=us-east-1
        fi
        tags=$(aws s3api get-bucket-tagging --bucket $bucket --region $region --query "TagSet[]" --output text 2>/dev/null | awk '{printf "{"$1": " $2"}"}')
        echo "$bucket | $tags" | awk 'NF<3' | tr -d '|' >> buckets_$$.txt
    done
}

# Get list of running instances
# All instances which were kicked off via qa_run_nimbus.sh
function get_ec2_instances() {
    aws ec2 describe-instances --filters "Name=tag-value,Values=qa_server" --query "Reservations[*].Instances[*].Tags[?Key=='Name'].Value" --output text > instances.txt
    uniq -c instances.txt
    rm instances.txt
}

# Dump cluster info
function dump_ec2() {
    ~/manage_aws.py --ec2-describe --key gpackard
}
