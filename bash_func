# Main ssh func, copies over dotfiles. Ignores .git and other hidden files
function gssh() {
    ssh $1 "mkdir ~/.packard_dotfiles 2>/dev/null"
    scp -qr ~/.packard_dotfiles/[!.]* $1:~/.packard_dotfiles/
    ssh -tAx $1 "bash --rcfile ~/.packard_dotfiles/bashrc"
}

# Custom ssh func for connecting to Raspberry Pi
function sumissh() {
    ssh gilly@sumi "mkdir ~/.packard_dotfiles 2>/dev/null"
    scp -qr ~/.packard_dotfiles/[!.]* gilly@sumi:~/.packard_dotfiles/
    ssh -tAx gilly@sumi "bash --rcfile ~/.packard_dotfiles/bashrc"
}

######################################################
# Vertica Functions
######################################################

# Helpful installer function shortcut
function install_rpm() {
    if [ -n "$1" ]; then
        /home/gpackard/installer.py install-vertica --hostlist="$HOSTLIST" --rpm-dir="$1" --ignore-install-fail
    else
        /home/gpackard/installer.py install-vertica --hostlist="$HOSTLIST" --ignore-install-fail
    fi
}

# Easy connect to db
function pvsql() {
    /opt/vertica/bin/vsql -p `admintools -t list_db -d "$1" | awk '/^Port/{print $2}'`
}

# Either tail vertica log, or open in vim
function vlog() {
    if [ "$1" = "-t" ]; then
        taillog=$(admintools -t list_db -d "${@:2}" | awk '/^Catalog/{print $3}')
        if [ -e ${taillog}/vertica.log ]; then
            tail -f "$taillog"/vertica.log
        else
            tail -f "$taillog"/vertica*log*
        fi
    else
        log=$(admintools -t list_db -d "$1" | awk '/^Catalog/{print $3}')
        if [ -e ${log}/vertica.log ]; then
            vim "$log"/vertica.log
        else
            vim "$log"/vertica*log*
        fi
    fi
}

# Jump to catalog dir
function catdir() {
    cd `admintools -t list_db -d "$1" | awk '/^Catalog/{print $3}'`
}

# Create 'regular' database
function pcreate() {
    /opt/vertica/bin/admintools -u -t create_db -d "$1" -s $HOSTLIST
}

# Nimbus create db func.
function ncreate() {
    is_cluster_ec2
    if [ "$?" = 0 ]; then
        /opt/vertica/bin/admintools -u -t create_db -d "$1" -s $HOSTLIST --shard-count="$2" --communal-storage-location=s3://nimbusdb/gpackard/"$1" --depot-path=/scratch_b/qa/"$1"_depot
    else
        /opt/vertica/bin/admintools -u -t create_db -d "$1" -s $HOSTLIST --get-aws-credentials-from-env-vars --shard-count="$2" --communal-storage-location=s3://nimbusdb/gpackard/"$1" --depot-path=/scratch_b/qa/"$1"_depot/
    fi
}

# Start db
function pstart() {
    /opt/vertica/bin/admintools -t start_db -d "$1"
}

# Stop db
function pstop() {
    /opt/vertica/bin/admintools -t stop_db -d "$1"
}

# Stop && Drop db
function pdrop() {
    /opt/vertica/bin/admintools -t stop_db -d "$1"
    /opt/vertica/bin/admintools -t drop_db -d "$1"
}

# Check if vertica db processes still running on all hosts in cluster
function dbps() {
    for host in ${HOSTLIST//,/ }
    do
        echo "---------------------------------------------------------------"
        echo "Host: $host"
        echo "---------------------------------------------------------------"
        ssh "$host" "ps -ef | grep $1 | grep vertica | grep -v ssh | grep -v grep"
    done
}

# Check disk space on all hosts in cluster.
# Useful for stress clusters
function dbdf() {
    if [ -d /vertica/data ]; then
        base='/vertica/data/'
    else
        base='/scratch_b/qa'
    fi

    for host in ${HOSTLIST//,/ }
    do
        echo "---------------------------------------------------------------"
        echo "Host: $host"
        echo "---------------------------------------------------------------"
        ssh "$host" "df -ah $base"
    done
}

# Run command across all hosts in cluster
function dbfunc() {
    for host in ${HOSTLIST//,/ }
    do
        echo "---------------------------------------------------------------"
        echo "Host: $host"
        echo "---------------------------------------------------------------"
        ssh "$host" "$@"
    done
}

# Copy binary and catalog directories up to S3
function s3_upload() {
    aws s3 cp /opt/vertica/bin/vertica ${1}/binary/
    dbfunc "find ${2}/*catalog -type d -exec aws s3 cp --recursive {} ${1}{} \;"
}

# Function for kicking off Nimbus stress tests
function run_stress() {
    cd $STRESS_VT
    nohup ./manage_stress_tests.pl --suite_name "$1" --cluster_hosts $HOSTLIST --failure_behavior=StopPassedDatabases --nimbus &
}

# Create a 6 node stress cluster without auto-running tests
function stress_cluster() {
    /scratch_b/qa/vertica/QA/tools/nimbus/qa_run_nimbus.sh -n 6 -s Stress_VT &
}

######################################################
# Misc. Functions
######################################################
function tf() {
    tail -f "$1"
}

function mkdircd() {
    mkdir "$1" && cd "$1"
}

function vtree() {
    pstree -alp $(whoami)
}

# Why did I put this in here....
function spongebob() {
    echo "$@" | tr -d ' ' | sed 's/\(.\)\(.\)/\u\1 \l\2 /g'
}

function parse_git_branch() {
    git branch 2> /dev/null | sed -e '/^[^*]/d' -e 's/* \(.*\)/ (\1)/'
}

function ide() {
    tmux split-window -v -p 30
}

######################################################
# AWS Functions
######################################################

# Get all untagged buckets for easy finger-pointing or deletion
function get_untagged_buckets() {
    for bucket in `aws s3api list-buckets --query "Buckets[*].Name" | tr -d \",[]`
    do
        region=$(aws s3api get-bucket-location --bucket $bucket --output text 2>/dev/null)
        # Returns 'None' for us-east-1.
        if [ "$region" == "None" ];then
            region=us-east-1
        fi
        tags=$(aws s3api get-bucket-tagging --bucket $bucket --region $region --query "TagSet[]" --output text 2>/dev/null | awk '{printf "{"$1": " $2"}"}')
        echo "$bucket | $tags" | awk 'NF<3' | tr -d '|' >> untagged_buckets.txt
    done
}

# Get all instances that don't have an Owner tag
function get_untagged_instances() {
    REGIONS="us-east-1,us-east-2,us-west-1,us-west-2"
    for region in ${REGIONS//,/ }
    do
        echo -e "\nRegion: $region" >> untagged_instances.txt
        aws ec2 describe-instances --region "$region" --output text --query 'Reservations[].Instances[?!not_null(Tags[?Key == `Owner`].Value)] | [].[InstanceId,Tags[?Key == `Name`].Value]' >> untagged_instances.txt
    done
}

# Get all EBS volumes that don't have an Owner tag
function get_untagged_volumes() {
    REGIONS="us-east-1,us-east-2,us-west-1,us-west-2"
    for region in ${REGIONS//,/ }
    do
        echo -e "\nRegion: $region" >> untagged_volumes.txt
        aws ec2 describe-volumes  --output text  --query 'Volumes[?!not_null(Tags[?Key == `Owner`].Value)] | [].[VolumeId,Size]' >> untagged_volumes.txt
    done
}

# Get qa stress buckets for easy finger-pointing or deletion
function get_buckets() {
    ~/manage_aws.py s3-describe -f Team=qa_server,DedicatedBucket=true
}

# Get list of instances owned by the qa_server team
function get_ec2() {
    ~/manage_aws.py ec2-describe -f Team=qa_server
}

# Dump GCP cluster info
function dump_gce() {
    ~/manage_gcp.py gce-describe -f Owner=gpackard --env-file
}

# Dump AWS cluster info
function dump_ec2() {
    ~/manage_aws.py ec2-describe -f Owner=gpackard --env-file
}

# Dump S3 bucket info
function dump_buckets() {
    ~/manage_aws.py s3-describe -f Owner=gpackard
}

# Return number of S3 buckets owned by entire company
function num_buckets() {
    num=$(aws s3 ls | wc -l)
    echo "$num total buckets"
}

# Return number of EC2 clusters owned by QA Server team
# Or, if provided, a particular user
function num_clusters() {
    ~/manage_aws.py ec2-describe -f Team=qa_server --log-level QUIET > ~/clusters.txt
    if [ -n "$1" ]; then
        num=$(grep "Cluster Name: $1.*" ~/clusters.txt | wc -l)
        grep "Cluster Name: $1.*" ~/clusters.txt
        echo -e "\n$1 owns $num total clusters"
    else
        num=$(grep "Cluster Name:" ~/clusters.txt | wc -l)
        grep "Cluster Name:" ~/clusters.txt | sort
        echo -e "\nqa_server owns $num total clusters"
    fi
    rm ~/clusters.txt
}

# Check if machines are up on EC2
function is_cluster_ec2() {
    if [ -f /sys/hypervisor/uuid ] && [ $(head -c 3 /sys/hypervisor/uuid) == 'ec2' ]; then
        return 0
    else
        return 1
    fi
}
